---
Author: Spa-Master
日期: 2026年1月17日
tags:
- topic/gpu
title: SM
slug: SM
---

流式多处理器
![Pasted image 20260104221619](https://cdn.jsdelivr.net/gh/HRbenY/blog-assets@master//img/20260117231048749.png)
上面是H100的一个SM，显示了 4_个子分区_，每个子分区包含一个张量核心、一个线程束调度器、一个寄存器文件以及不同精度的 CUDA 核心组。

## Memmory

除了计算单元之外，GPU 还具有内存层次结构，其中最大的是 HBM（GPU 主内存），然后是一系列较小的缓存（L2、L1/SMEM、TMEM、寄存器内存）。

- **寄存器：** 每个子分区都有自己的寄存器文件，其中包含 H100/B200（每个 SM）上的 16,384 个 32 位字，`4 * 16384 * 4 = 256kiB`CUDA 内核可以访问该文件。
    - 每个 CUDA 核心一次最多只能访问 256 个寄存器，因此尽管我们可以为每个 SM 调度多达 64 个“驻留线程束”，但`256 * 1024 / (4 * 32 * 256)`如果每个线程使用 256 个寄存器，则一次只能容纳 8 个线程束。
- **SMEM（L1缓存）**：每个SM都有其自身的256kB片上缓存，称为SMEM。SMEM既可以由程序员控制作为“共享内存”，也可以由硬件用作片上缓存。SMEM用于存储TC矩阵乘法器的激活值和输入值。
    
- **L2缓存**：所有SM共享9使用相对较大的约 50MB L2 缓存来减少主内存访问。
    - 它的大小与TPU的VMEM类似，但速度慢**得多**，而且不受程序员控制。这就导致了一种“远程控制”的现象：程序员需要修改内存访问模式，以确保L2缓存得到充分利用。10
    - NVIDIA并未公布其芯片的L2带宽，但经测量约为5.5TB/s。这大约是HBM带宽的1.6倍，但由于是全双工模式，因此实际双向带宽接近3倍。相比之下，TPU的VMEM容量是其两倍，带宽也更高（约为40TB/s）。
- **HBM：** GPU 主内存，用于存储模型权重、梯度、激活值等。
    - HBM 容量从 Volta 的 32GB 大幅增加到 Blackwell (B200) 的 192GB。
    - 从 HBM 到 CUDA Tensor Core 的带宽称为 HBM 带宽或内存带宽，在 H100 上约为 3.35TB/s，在 B200 上约为 9TB/s。
