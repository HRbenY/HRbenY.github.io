---
title: Tokenizer
author: Spa-Master
date: '2026-01-17 00:00:00'
updated: '2026-01-17 00:00:00'
tags:
- topic/transformer
- type/note
---

## 1. 什么是分词器 (Tokenizer)？

模型无法直接理解人类的文本（字符串），它只能处理数字。分词器的作用就是将文本转换成模型可以理解的数字序列。

- **流程**：输入文本 $\rightarrow$ 切分成 Token (词元) $\rightarrow$ 映射为 ID (索引数字) $\rightarrow$ 输入模型。
    
- **Token 是什么**：Token 可以是一个完整的单词（如 `is`），一个单词的一部分（如 `ization`），甚至是一个字符或空格。
    
- **主流算法**：GPT 系列主要使用的是 **BPE (Byte Pair Encoding)** 算法，旨在通过合并高频出现的字符组合来压缩文本，提高效率。
    
---

## 2. 截图场景解析与局限性 (Limitations)

截图中的不同色块代表了分词器将文本切分后的不同 Token。我们可以从图中展示的四个特定场景，分析 LLM 在分词层面遇到的“怪异现象”和限制：
![Pasted image 20251228171806](https://cdn.jsdelivr.net/gh/HRbenY/blog-assets@master//img/20260117145252696.png)  
### 场景一：算术与数字处理 (Arithmetic)

> 截图内容：127 + 677 = 804
> 
> 现象：数字通常不会被识别为一个整体，而是被切碎。

- **局限性**：**破坏了数值的位值概念。**
    
    - 在 GPT-2（以及许多早期模型）的分词器中，数字往往被切分成不符合直觉的片段。例如，`1275` 可能被切分为 `12` 和 `75`，或者 `1`、`2`、`7`、`5`。
        
    - **后果**：模型很难学会“进位”等数学逻辑，因为对它来说，这更像是文本拼接预测，而不是数值计算。这就是为什么 LLM 做加减法经常一本正经胡说八道的原因之一。
        
    - **改进**：较新的模型（如 LLaMA 或 GPT-4）尝试将数字按每一位单独分词，以提高数学能力。
        

### 场景二：大小写敏感 (Case Sensitivity)

> 截图内容：Egg, I have an Egg, egg, EGG
> 
> 现象：Egg (首字母大写)、egg (全小写)、EGG (全大写) 被赋予了不同的颜色，意味着它们是完全不同的 Token。

- **局限性**：**词表浪费与语义分散。**
    
    - 虽然这三个词语义相同，但在模型眼里它们是三个完全不同的 ID。模型必须分别学习这三个 Token 的含义。
        
    - **后果**：这浪费了宝贵的词表空间（Vocabulary Size），并且如果某个变体（如全大写 `EGG`）在训练数据中出现很少，模型对它的理解就会比 `egg` 差。
        

### 场景三：非英语/多语言支持 (Non-English / Multilingual)

> 截图内容：韩语段落 만나서 반갑습니다...
> 
> 现象：韩语文本的色块非常细碎，几乎每个字甚至每个字的部件都被切成了不同的 Token。

- **局限性**：**信息密度低，上下文窗口利用率差。**
    
    - GPT-2 的词表主要是针对英语优化的。对于韩语、中文等非拉丁语系，分词器往往找不到对应的“整词”，只能回退到按字节或字符切分。
        
    - **后果**：
        
        1. **更贵**：同样的句子，中文或韩语消耗的 Token 数量远多于英语（在 GPT-2/3 时代尤其明显），调用 API 时成本更高。
            
        2. **更“短”**：因为 Token 数量多，同样的上下文窗口（Context Window）能容纳的非英语内容就变少了。
            

### 场景四：代码与格式 (Code & Formatting)

> 截图内容：Python 代码 for i in range(1, 101):
> 
> 现象：缩进（空格）、关键词（for, in）、标点符号都被切分。

- **局限性**：**对格式极其敏感。**
    
    - Python 强依赖缩进。如果分词器将 (4个空格) 识别为一个 Token，而将 (2个空格) 识别为另一个，模型就需要精确预测使用哪个 Token 来保证代码运行。
        
    - 像 `FizzBuzz` 这样的变量名，因为不是通用单词，会被拆解为 `Fizz` + `Buzz`。如果代码中有很多生僻变量名，会显著增加 Token 消耗。

### 对比分析：GPT-2 (`gpt2`) vs GPT-4 (`cl100k_base`)

OpenAI 的模型经历了从 `gpt2` (用于 GPT-2 和 GPT-3) 到 `cl100k_base` (用于 GPT-3.5-Turbo 和 GPT-4) 的升级。这次分词器的升级对性能和成本有直接影响。
![Pasted image 20251228172350](https://cdn.jsdelivr.net/gh/HRbenY/blog-assets@master//img/20260117145252697.png)

| **特性**   | **GPT-2 Tokenizer (gpt2 / r50k_base)**    | **GPT-4 Tokenizer (cl100k_base)**                     |
| -------- | ----------------------------------------- | ----------------------------------------------------- |
| **词表大小** | **约 50,000 个**                            | **约 100,000 个**                                       |
| **中文支持** | **较差**。一个汉字通常被切分为 2-3 个 Token（甚至更多字节级碎片）。 | **优秀**。词表更大，包含了更多常用汉字。许多常见汉字现在是一个单独的 Token。           |
| **代码处理** | **一般**。对空格、缩进和常见代码关键字的优化不足。               | **大幅优化**。针对代码中的常见模式（如连续空格、特定函数名）进行了专门优化，压缩率更高。        |
| **压缩效率** | 较低。同样的文本需要更多的 Token 来表示。                  | **较高**。同样的文本，Token 数量平均减少 **15% - 20%**（英文），中文减少幅度更大。 |
| **数字处理** | 经常将数字切碎且不规律（如 `1275` -> `12`, `75`）。      | 稍微一致一些，但依然不是完美的按位切分（这主要还是 BPE 的通病）。                   |
### 核心差异总结：

1. **省钱 & 扩容**：`cl100k_base` 最直接的好处是**更省 Token**。因为它的词表更大（100k），它能“一口吃下”更复杂的词，而不需要切得那么碎。这意味着对于同样的 prompt，使用 GPT-4 往往比 GPT-3 花费的 Token 更少，且能塞进上下文窗口的内容更多。
    
2. **多语言与代码能力增强**：由于在训练分词器时加入了更多代码和多语言语料，`cl100k_base` 不再像 GPT-2 那样“歧视”非英语内容，这对中文开发者和程序员非常友好。
## 3. 主流的分词器类型 (Tokenizer Types)

虽然目前 GPT 系列主要使用 BPE，但在 NLP 领域主要有三种主流的分词算法，它们的核心目标都是**平衡词表大小（Vocabulary Size）与文本覆盖率**，尽量避免出现“未知词（[UNK]）”。

### 3.1 BPE (Byte Pair Encoding)

- **代表模型**：**GPT 系列 (GPT-2, GPT-3, GPT-4)**, RoBERTa, LLaMA。
    
- **原理**：统计语料中字符组合出现的频率。从单个字符开始，不断**合并出现频率最高**的相邻字符对，直到词表达到预设大小。
    
- **特点**：
    
    - 能很好地处理生僻词（通过拆解为子词）。
        
    - GPT 使用的是 **Byte-level BPE**（基于字节而非字符），这意味着它可以处理任何 Unicode 字符串（包括 emoji 和各种奇怪符号），不会出现 [UNK]。
        

### 3.2 WordPiece

- **代表模型**：**BERT**, DistilBERT。
    
- **原理**：与 BPE 类似，也是合并子词。但它在选择合并哪两个子词时，不是看频率最高，而是看**合并后能最大程度增加训练数据语言模型的似然概率 (Likelihood)**。
    
- **特点**：通常比 BPE 稍微紧凑一些，但计算量稍大。
    

### 3.3 Unigram Language Model

- **代表模型**：**T5**, ALBERT, XLNet (使用 SentencePiece 库)。
    
- **原理**：思路与 BPE 相反。BPE 是“从下往上”合并，Unigram 是“从上往下”剪枝。它先初始化一个巨大的词表，然后计算丢弃某些词对整体损失函数的影响，**保留概率最高**的子词，剔除贡献小的，直到达到预设词表大小。
    
- **特点**：它是基于概率的模型，对于同一个句子可以产生多种分词可能（Subword Regularization），这在训练时有时能增加模型的鲁棒性。

---

## 4 相关资料

[Programmer's intro to unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)
