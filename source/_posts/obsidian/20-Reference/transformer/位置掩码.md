---
title: 位置掩码
author: Spa-Master
date: '2026-01-17 00:00:00'
updated: '2026-01-17 00:00:00'
tags:
- lang/python
- topic/transformer
- type/note
cover: /img/covers/auto/obsidian-20-reference-transformer-md-8d03a0ff7e.png
---

对于语言模型，我们需要在训练时赋予因果遮蔽，避免模型使用未来的token训练。所以对时间序列维度要进行位置掩码。

## 串行做法

```python
xbow = torch.zeros((B, T, C))

for b in range(B):

    for t in range(T):

        x_prev = x[b, :t + 1]

        xbow[b, t] = torch.mean(x_prev, 0)
```
这里是一个很简单的token交互，取平均值。可以看到在每个batch维度独立地对时间序列进行遮蔽。对于第t组数据（B, 1, C），我们只让他看到过去的0~t时刻的token内容，即`x[b, :t + 1]` 。

串行实现逻辑很简单，但是对于GPU不友好。我们可以引入一个数学技巧来处理时间遮蔽。

## 矩阵做法

```python
torch.manual_seed(1337)

B, T, C = 4, 8, 32

x = torch.randn(B, T, C)

tril = torch.tril(torch.ones(T, T))

wei = wei.masked_fill(tril == 0, float('-inf'))

wei = F.softmax(wei, dim=2)

out = wei @ x
```

使用下三角矩阵左乘x，广播机制会自动在batch维度上广播，所以先忽略batch维度。
wei的维度为(T, T)， x的维度为(T, C)。wei @ x会得到一个(T, C)的矩阵。如果wei是下三角的情况下，假设 $T=3$，输入序列 $X$ 包含三个时刻的向量 $x_0, x_1, x_2$（每个向量维度为 $C$）：

$$W =
\left[
\begin{array}{ccc}
1    & 0    & 0 \\
0.5  & 0.5  & 0 \\
0.33 & 0.33 & 0.33
\end{array}
\right]$$
$$
W X =
\left[
\begin{array}{ccc}
1    & 0    & 0 \\
0.5  & 0.5  & 0 \\
0.33 & 0.33 & 0.33
\end{array}
\right]
\left[
\begin{array}{c}
x_0 \\
x_1 \\
x_2
\end{array}
\right]
=
\left[
\begin{array}{c}
1 \cdot x_0 \\
0.5 \cdot x_0 + 0.5 \cdot x_1 \\
0.33 \cdot x_0 + 0.33 \cdot x_1 + 0.33 \cdot x_2
\end{array}
\right]
$$
通过这种矩阵乘法，结果矩阵的第 $t$ 行，正是输入 $X$ 从第 0 行到第 $t$ 行的**平均值**。


值得注意的是，在某些场景下可能不需要进行因果遮蔽。比如语句的情感推断。在这种情况下需要全部上下文的信息进行推理。
