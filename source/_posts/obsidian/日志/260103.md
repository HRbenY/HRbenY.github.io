---
title: '260103'
author: Spa-Master
date: '2026-01-12 21:08:06'
updated: '2026-01-12 21:08:06'
tags: []
---

## transformer

- 定位到内存泄漏问题

```bash
Step 0 | Loss: 3.62174
[step 0] RSS=20.0664 MB
rss MB: pre 18.6914 fwd 18.6914 bwd 19.3164 opt 20.0664
[backward calls 100] alive TensorImpl=7765 Node=7765 | created TensorImpl=14530 Node=14530
Step 100 | Loss: 2.56172
[step 100] RSS=180.066 MB
rss MB: pre 179.691 fwd 179.691 bwd 180.066 opt 180.066
[backward calls 200] alive TensorImpl=15465 Node=15465 | created TensorImpl=28930 Node=28930
Step 200 | Loss: 2.54764
[step 200] RSS=340.941 MB
rss MB: pre 340.566 fwd 340.566 bwd 340.941 opt 340.941
[backward calls 300] alive TensorImpl=23165 Node=23165 | created TensorImpl=43330 Node=43330
Step 300 | Loss: 2.50827
[step 300] RSS=501.691 MB
rss MB: pre 501.316 fwd 501.316 bwd 501.691 opt 501.691
[backward calls 400] alive TensorImpl=30865 Node=30865 | created TensorImpl=57730 Node=57730
Step 400 | Loss: 2.53477
[step 400] RSS=662.566 MB
rss MB: pre 662.191 fwd 662.191 bwd 662.566 opt 662.566
[backward calls 500] alive TensorImpl=38565 Node=38565 | created TensorImpl=72130 Node=72130
Step 500 | Loss: 2.59063
[step 500] RSS=823.316 MB
rss MB: pre 823.066 fwd 823.066 bwd 823.316 opt 823.316
```

原因是反向传播的lambda捕获问题，目前没有一个一个修改，而是给Tensor的backward函数增添断开非叶子节点计算图的功能。

修改后：

```bash
Step 0 | Loss: 3.67255
[step 0] RSS=18.8359 MB
rss MB: pre 18.5859 fwd 18.5859 bwd 18.5859 opt 18.8359
[backward calls 50] alive TensorImpl=69 Node=46 | created TensorImpl=7330 Node=7330
[backward calls 100] alive TensorImpl=69 Node=46 | created TensorImpl=14530 Node=14530
Step 100 | Loss: 2.57454
[step 100] RSS=19.5859 MB
rss MB: pre 19.5859 fwd 19.5859 bwd 19.5859 opt 19.5859
[backward calls 150] alive TensorImpl=69 Node=46 | created TensorImpl=21730 Node=21730
[backward calls 200] alive TensorImpl=69 Node=46 | created TensorImpl=28930 Node=28930
Step 200 | Loss: 2.98733
[step 200] RSS=19.5859 MB
rss MB: pre 19.5859 fwd 19.5859 bwd 19.5859 opt 19.5859
[backward calls 250] alive TensorImpl=69 Node=46 | created TensorImpl=36130 Node=36130
[backward calls 300] alive TensorImpl=69 Node=46 | created TensorImpl=43330 Node=43330
Step 300 | Loss: 2.74883
[step 300] RSS=19.5859 MB
rss MB: pre 19.5859 fwd 19.5859 bwd 19.5859 opt 19.5859
[backward calls 350] alive TensorImpl=69 Node=46 | created TensorImpl=50530 Node=50530
[backward calls 400] alive TensorImpl=69 Node=46 | created TensorImpl=57730 Node=57730
Step 400 | Loss: 2.78805
[step 400] RSS=19.5859 MB
rss MB: pre 19.5859 fwd 19.5859 bwd 19.5859 opt 19.5859
[backward calls 450] alive TensorImpl=69 Node=46 | created TensorImpl=64930 Node=64930
[backward calls 500] alive TensorImpl=69 Node=46 | created TensorImpl=72130 Node=72130
Step 500 | Loss: 3.31525
[step 500] RSS=19.5859 MB
rss MB: pre 19.5859 fwd 19.5859 bwd 19.5859 opt 19.5859
```

### 重构计划

内存泄漏问题还未根治，下一步是将grad和node从TensorImpl中分离开，单独包装到AutogradMeta结构体中，并加上grad开关{。

- [ ] AutoGradMeta定义与迁移
- [ ] TensorImpl适配
	- [ ] 修改结构体
	- [ ] 添加辅助函数
	     ```c++
	      // 1. 只读访问（可能为空） 
	    AutogradMeta* get_meta() const { return autograd_meta.get(); } 
	      // 2. 获取或创建（用于准备写梯度或记录图时） 
		AutogradMeta& materialize_meta() { 
			if (!autograd_meta) autograd_meta = std::make_unique<AutogradMeta>();
			return *autograd_meta; 
		}
	     ```
- [ ] 所有Backward适配
	- [ ] 捕获列表现在不捕获TensorImpl(这是错的，会导致内存泄漏)，而是捕获AutoGradMeta week_ptr
	- [ ] 内部梯度逻辑修改
- [ ] Tensor文件中所有涉及访问TensorImpl的API的适配
- [ ] 优化器梯度更新适配
